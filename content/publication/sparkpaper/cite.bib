@article{sparkpaper,
title = {Econometrics at Scale: Spark up Big Data in Economics},
journal = {Journal of Data Science},
volume = {54},
pages = {100879},
year = {2022},
issn = {1572-3089},
doi = {https://doi.org/10.6339/22-JDS1035},
url = {https://www.sciencedirect.com/science/article/pii/S1572308921000395},
author = {Benjamin Bluhm, Jannic Cutura},
keywords = {Apache Spark; distributed computing; econometrics},
abstract = {This paper provides an overview of how to use "big data" for social science research (with an
emphasis on economics and finance). We investigate the performance and ease of use of different
Spark applications running on a distributed file system to enable the handling and analysis of
data sets which were previously not usable due to their size. More specifically, we explain how to
use Spark to (i) explore big data sets which exceed retail grade computers memory size and (ii)
run typical statistical/econometric tasks including cross sectional, panel data and time series
regression models which are prohibitively expensive to evaluate on stand-alone machines. By
bridging the gap between the abstract concept of Spark and ready-to-use examples which can
easily be altered to suite the researchers need, we provide economists and social scientists more
generally with the theory and practice to handle the ever growing datasets available. The ease
of reproducing the examples in this paper makes this guide a useful reference for researchers
with a limited background in data handling and distributed computing.}
